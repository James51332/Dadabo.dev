---
layout: post
title: Revisiting My Ocean Simulation
---

It's been a fair bit of time since my first attempt at simulating the ocean through a sum of sines approach. Now, I'm ready to tackle a more daunting, but much more appealing approach using the Fourier transform!

<!--more-->

### Where We Left Off

This is the third post relevant to our journey to simulate ocean water. The [first post](https://dadabo.dev/2024/03/31/waves.html) discusses the process of transforming a flat plane into a surface that vaguely resembles water by adding waves of various directions, frequencies, and amplitudes together to offset its vertices. The [second post](https://dadabo.dev/2024/04/11/fourier.html) discusses the Fourier transform, and is an intuitive introduction to the method we will use to enhance our ocean!

Right now, our understanding of the Fourier transform is that it is a formula to convert between a signal's *time domain* and its *frequency domain* (also known as its *spectrum*). We can also take the inverse transform to add back together all of the waves. For our ocean simulation we can define a spectrum that is obtained from real oceanographic data, and use the inverse transform to generate a surface height map.

However, let's take a moment to consider how this is different from what we did before. Both approaches sum together a variety of waves, but the spectrum for the sum of sines was generated using fractal brownian motion. So why do we need the Fourier transform? The answer lies in the time complexity of the algorithm. 

Consider that as the size of our ocean grows (to be convincing, it needs to), the shader must add up waves for every point displaced. If the ocean is $n$ x $n$ vertices, the number of calculations is proportional to $n^2$. Therefore, we consider the algorithm $O(n^2)$, pronounced $O$ of $n^2$. As we scale our ocean, the number of computations grows too quickly to be reasonable. Here's where the Fourier transform comes in: *Using symmetry revealed by waves in the complex plane, the Fourier transform's time complexity can be reduced!* What a mouthful!

<details>
  <summary>Sidenote</summary>
  
  <h3>Some Technicalities</h3>

  <p>While it would technically be possible to perform this exact same optimization on our sum of sines approach (see <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform#Computation">Discrete Cosine Transform</a>), there are a few reasons not to do this. First off, the Fourier transform is much easier to work with. We are going to need to pre calculate the surface height map of our water to a texture to calculate it efficiently, and we want the edges of our texture to align so that it can be tiled in a grid if we choose. Our DFT will be periodic by definition, but other transforms don't play so nicely due to the even and odd nature of cosine and sine waves.</p>
  
  <p>Additionally, other Fourier-related transforms are typically implemented using a fast Fourier transform (FFT). The other transforms are only used because they have more purpose in other fields. For example, lossy image compression by JPEG is done using the DCT because it leads to a more dense spectrum than a DFT, and less memory can be used to create a better recreation of the image. Since we are likely going to need to implement the FFT anyways, it's very natural to use it as a our sole transform.</p>
</details>

## Unlocking the Fast Fourier Transform

*Disclaimer: This section covers the method of calculating the sum of our waves in real-time. The content is dense and math-heavy, and isn't needed to appreciate the ocean simulation.*

The first thing to note is that the formula for the DFT and IDFT are so close, and any optimization we make to one can be made to the other. Still, at an initial glance, both have a time complexity of $O(n^2)$. So how can we exploit symmetry? It's not very intuitive to visualize, since there are so many different waves being added together. We'll turn to the formula for the IDFT, since this is what our code will need to do. Remember that $x(n)$ is a sample of the time-domain at time $n$, $X(k)$ is the amplitude of the wave with frequency $k$, and $N$ is the number of samples in our signal:

$$ x(n) = \sum^{N-1}_{k=0} X(k) e^{2\pi i kn/N}$$

We know that the Fourier transform is periodic over $N$ samples in the time-domain by definition. This means that if we add $N$ to the time, the expression should have the same value. Let's build off of this idea. We can look a half period ahead, and see where this leads us. With a little help from the properties of exponents, as well as Euler's formula, the new expression is pleasantly familiar.

$$ \begin{align} x(n + N/2) &= \sum^{N-1}_{k=0} X(k) e^{2\pi i k(n+N/2)/N} \\
                            &= \sum^{N-1}_{k=0} X(k) e^{2\pi i k(n)/N} e^{\pi i k} \\
                            &= \sum^{N-1}_{k=0} (-1)^k X(k) e^{2\pi i k(n)/N} \end{align}$$

The only difference between this and the initial expression for $x(n)$ is that the signs of some terms are flipped. More precisely, all of the odd terms are subtracted. But if we are clever, this doesn't stop us from making an optimization. We'll add all of our even terms together and store the sum, and do the same for our odd terms. To find $x(n)$, we add these two sums. To find $x(n + N/2)$, we take their difference. In practice, this is a nearly free way to save computing steps.

This is a great start. We've managed to cut the number of computations in half, and this really could significantly reduce our computation time. But this isn't improving our time complexity. As we increase the number of frequencies, the number of calculations will still grow too fast, and crunching the numbers twice as fast just may not cut it. We need to keep the algorithm fast as scale grows, by keeping the number of computations low.

It's not immediately clear how to do this, so let's keep investigating our previous ideas and see where that leads us. We will take a closer look at the sums for our even terms, and then for our odd terms. Interestingly, it's not very straightforward to separate our even terms and odd terms using the just equation we derived before. One method would be to list out some even terms, and derive a generic formula. The approach I prefer is to replace $k$ with $2k$, and cut the number of samples in half. This leads us to this equation:

$$ \text{sum of even terms} = \sum_{k=0}^{N/2-1} X(2k) e^{2\pi i (2kn)/N}$$

Again, we remarkably get something eerily similar to the initial IDFT. There are two major differences. First off, we have half as many samples. However, the wave exponent doesn't reflect this. Conveniently, both our numerator and denominator are twice as large as we'd like them. All we have to do is cut them both in half, and our problem is solved:

$$ \text{sum of even terms} = \sum_{k=0}^{N/2-1} X(2k) e^{2\pi ik(n)/(N/2)}$$

The only other difference is that we only sample the frequency of our even terms. This is actually pretty easy to get around, by placing the even terms next to each other, so that they can again be accessed with index $k$. What we are left with is a DFT! Let's do the same thing for the odd terms. We can index them by simply adding one to the indices for all of the even terms. This one doesn't play as gently, but we can break apart the 

$$ \begin{align} \text{sum of odd terms} &= \sum_{k=0}^{N/2-1} X(2k+1) e^{2\pi i(2k+1)(n)/(N)} \\
                                         &= e^{2\pi i n/N} \sum_{k=0}^{N/2-1} X(2k+1) e^{2\pi ik(n)/(N/2)} \end{align}$$

Thankfully, the term we factor out doesn't depend on the frequency, so we only have to calculate it once. Then, once we solve the same problems as for the evens, we get another DFT! Here is our insight: *The DFT can be calculated using two half-length DFTs and a small combination step!*. Alone, this isn't much, but we can recursively apply this rule. Therefore, a large DFT may be split into two medium DFTs, which are again divided into four small DFTs. This repeated divide-and-conquer algorithm is known as the fast Fourier Transform (FFT). Our pseudo-code inverse FFT implementation may look something like this, and can be implemented using a recursive algorithm.

```js
function IFFT(amplitudes)
{
  N = amplitudes.length

  // base case (amplitude = signal)
  if (N == 1) return amplitudes[0]

  // assert the number of samples is otherwise even
  assert(N % 2 == 0)

  // run the transform on the evens and the odds
  evens = IFFT(amplitudes.evens)
  odds = IFFT(ampltidues.odds)

  // combine the two transforms together to get our result
  for (n in N/2)
  {
    // multiply odd factor by twiddle factor
    odds[n] *= eulers(2 * PI * n / N)

    // reuse the amplitudes array for our final values
    amplitudes[n] = evens[n] + odds[n]
    amplitudes[n + N/2] = evens[n] - odds[n]
  }

  return amplitudes
}
```

Wonderful! Again, there are so many ways to take this further. For now, we only support inputs with lengths of powers of two. This is okay, but there are versions of the FFT that support all sorts of lengths, and it'd be interesting to explore how these work. Another problem that we face is that our program is moderately optimized for computations, but it's not optimized for memory usage. Each stage allocates (depending on implementation of the pseudo-code) two arrays, but it's possible to use only a single array for the entire algorithm, cleverly swapping around values using a [butterfly network](https://en.wikipedia.org/wiki/Butterfly_network). This is how I've implemented the algorithm to run in parallel on the GPU. Another thing to consider is the FFT in multiple dimensions, which is necessary for our ocean. 

There is no shortage of rabbit holes to investigate, but a final one I'd like to touch on is the time complexity of the FFT. Let's treat our algorithm as a series of combinations of DFTs. No matter what set of combinations we are on, every signal will be combined with exactly one other to produce two new results, so we'll always have $N/2$ combinations. The number of sets of combinations is based on the number of times we have to double our signal size. This is equal to $log_2(n)$. Therefore, the computation complexity is equal to the number of combinations per set times the number of sets, which is $O(nlog(n))$. This is known as loglinear complexity, and is perfectly acceptable for our use case. It's been a long journey, but for now, we are done with the Fourier transform!